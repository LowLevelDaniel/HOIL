/**
 * @file lexer.c
 * @brief Implementation of the lexical analyzer for HOIL
 * 
 * This file implements the lexer that breaks down HOIL source code
 * into tokens that can be processed by the parser.
 *
 * @author Generated by Claude
 * @date 2025-03-13
 */

#include "lexer.h"
#include "error_handling.h"
#include "common.h"
#include <string.h>
#include <ctype.h>
#include <assert.h>
#include <errno.h>
#include <limits.h>

/**
 * @brief Maximum token length to prevent buffer overflow
 */
#define MAX_TOKEN_LENGTH (64 * 1024)  // 64KB token limit

/**
 * @brief Lexer structure definition
 */
struct lexer_t {
  const char* source;      /**< Source code being lexed */
  size_t source_length;    /**< Length of source code */
  const char* filename;    /**< Source filename for error reporting */
  
  const char* current;     /**< Current position in source */
  size_t line;             /**< Current line number */
  size_t column;           /**< Current column number */
  
  token_t peeked_token;    /**< Token buffer for peek operations */
  bool has_peeked_token;   /**< Whether there's a buffered token */
  bool has_error;          /**< Whether an error occurred */
};

/**
 * @brief Keyword lookup entry
 */
typedef struct {
  const char* keyword;
  token_type_t type;
} keyword_t;

/**
 * @brief Table of HOIL keywords and their token types
 */
static const keyword_t keywords[] = {
  {"MODULE", TOKEN_MODULE},
  {"TYPE", TOKEN_TYPE},
  {"CONSTANT", TOKEN_CONSTANT},
  {"GLOBAL", TOKEN_GLOBAL},
  {"FUNCTION", TOKEN_FUNCTION},
  {"EXTERN", TOKEN_EXTERN},
  {"TARGET", TOKEN_TARGET},
  {"ENTRY", TOKEN_ENTRY},
  {"RET", TOKEN_RET},
  {"BR", TOKEN_BR},
  {"CALL", TOKEN_CALL},
  {"ADD", TOKEN_ADD},
  {"SUB", TOKEN_SUB},
  {"MUL", TOKEN_MUL},
  {"DIV", TOKEN_DIV},
  {"REM", TOKEN_REM},
  {"AND", TOKEN_AND},
  {"OR", TOKEN_OR},
  {"XOR", TOKEN_XOR},
  {"NOT", TOKEN_NOT},
  {"SHL", TOKEN_SHL},
  {"SHR", TOKEN_SHR},
  {"CMP_EQ", TOKEN_CMP_EQ},
  {"CMP_NE", TOKEN_CMP_NE},
  {"CMP_LT", TOKEN_CMP_LT},
  {"CMP_LE", TOKEN_CMP_LE},
  {"CMP_GT", TOKEN_CMP_GT},
  {"CMP_GE", TOKEN_CMP_GE},
  {"LOAD", TOKEN_LOAD},
  {"STORE", TOKEN_STORE},
  {"LEA", TOKEN_LEA},
  {"FENCE", TOKEN_FENCE},
  {"CONVERT", TOKEN_CONVERT},
  {"TRUNC", TOKEN_TRUNC},
  {"EXTEND", TOKEN_EXTEND},
  {"VADD", TOKEN_VADD},
  {"VDOT", TOKEN_VDOT},
  {"VSPLAT", TOKEN_VSPLAT},
  {"VLOAD", TOKEN_VLOAD},
  {"ATOMIC_ADD", TOKEN_ATOMIC_ADD},
  {"ATOMIC_CAS", TOKEN_ATOMIC_CAS},
  {"ALWAYS", TOKEN_ALWAYS},
  {NULL, TOKEN_EOF}
};

/**
 * @brief Convert token type to string for debugging
 * 
 * @param type Token type
 * @return String representation of token type
 */
const char* token_type_to_string(token_type_t type) {
  switch (type) {
    case TOKEN_EOF: return "EOF";
    case TOKEN_IDENTIFIER: return "IDENTIFIER";
    case TOKEN_NUMBER: return "NUMBER";
    case TOKEN_STRING: return "STRING";
    case TOKEN_MODULE: return "MODULE";
    case TOKEN_TYPE: return "TYPE";
    case TOKEN_CONSTANT: return "CONSTANT";
    case TOKEN_GLOBAL: return "GLOBAL";
    case TOKEN_FUNCTION: return "FUNCTION";
    case TOKEN_EXTERN: return "EXTERN";
    case TOKEN_TARGET: return "TARGET";
    case TOKEN_ENTRY: return "ENTRY";
    case TOKEN_RET: return "RET";
    case TOKEN_BR: return "BR";
    case TOKEN_CALL: return "CALL";
    case TOKEN_ADD: return "ADD";
    case TOKEN_SUB: return "SUB";
    case TOKEN_MUL: return "MUL";
    case TOKEN_DIV: return "DIV";
    case TOKEN_REM: return "REM";
    case TOKEN_AND: return "AND";
    case TOKEN_OR: return "OR";
    case TOKEN_XOR: return "XOR";
    case TOKEN_NOT: return "NOT";
    case TOKEN_SHL: return "SHL";
    case TOKEN_SHR: return "SHR";
    case TOKEN_CMP_EQ: return "CMP_EQ";
    case TOKEN_CMP_NE: return "CMP_NE";
    case TOKEN_CMP_LT: return "CMP_LT";
    case TOKEN_CMP_LE: return "CMP_LE";
    case TOKEN_CMP_GT: return "CMP_GT";
    case TOKEN_CMP_GE: return "CMP_GE";
    case TOKEN_LOAD: return "LOAD";
    case TOKEN_STORE: return "STORE";
    case TOKEN_LEA: return "LEA";
    case TOKEN_FENCE: return "FENCE";
    case TOKEN_CONVERT: return "CONVERT";
    case TOKEN_TRUNC: return "TRUNC";
    case TOKEN_EXTEND: return "EXTEND";
    case TOKEN_VADD: return "VADD";
    case TOKEN_VDOT: return "VDOT";
    case TOKEN_VSPLAT: return "VSPLAT";
    case TOKEN_VLOAD: return "VLOAD";
    case TOKEN_ATOMIC_ADD: return "ATOMIC_ADD";
    case TOKEN_ATOMIC_CAS: return "ATOMIC_CAS";
    case TOKEN_ALWAYS: return "ALWAYS";
    case TOKEN_LPAREN: return "(";
    case TOKEN_RPAREN: return ")";
    case TOKEN_LBRACE: return "{";
    case TOKEN_RBRACE: return "}";
    case TOKEN_LBRACKET: return "[";
    case TOKEN_RBRACKET: return "]";
    case TOKEN_COMMA: return ",";
    case TOKEN_SEMICOLON: return ";";
    case TOKEN_COLON: return ":";
    case TOKEN_DOT: return ".";
    case TOKEN_ARROW: return "->";
    case TOKEN_EQUALS: return "=";
    case TOKEN_ASTERISK: return "*";
    case TOKEN_AMPERSAND: return "&";
    case TOKEN_LESS: return "<";
    case TOKEN_GREATER: return ">";
    case TOKEN_ERROR: return "ERROR";
    default: return "UNKNOWN";
  }
}

/**
 * @brief Free resources owned by a token
 *
 * @param token Token to clean up
 */
void token_destroy(token_t* token) {
  if (token == NULL) {
    return;
  }
  
  // Free owned resources based on token type
  if (token->type == TOKEN_STRING && token->string_value != NULL) {
    free(token->string_value);
    token->string_value = NULL;
  } else if (token->type == TOKEN_IDENTIFIER && token->identifier_value != NULL) {
    free(token->identifier_value);
    token->identifier_value = NULL;
  }
  
  // Zero out the token to prevent double-free
  token->type = TOKEN_ERROR;
  token->start = NULL;
  token->length = 0;
}

/**
 * @brief Copy a token
 *
 * @param token Token to copy
 * @return Copy of token (caller must free with token_destroy)
 */
token_t token_copy(const token_t* token) {
  token_t copy;
  
  if (token == NULL) {
    // Return an error token if input is NULL
    copy.type = TOKEN_ERROR;
    copy.start = NULL;
    copy.length = 0;
    copy.line = 0;
    copy.column = 0;
    copy.string_value = NULL;
    return copy;
  }
  
  // Copy basic fields
  copy.type = token->type;
  copy.start = token->start;
  copy.length = token->length;
  copy.line = token->line;
  copy.column = token->column;
  
  // Copy type-specific data
  switch (token->type) {
    case TOKEN_STRING:
      if (token->string_value != NULL) {
        copy.string_value = safe_strdup(token->string_value);
        if (copy.string_value == NULL) {
          // Failed to allocate memory, return error token
          error_report(ERROR_MEMORY, "Failed to allocate memory for string token copy");
          copy.type = TOKEN_ERROR;
        }
      } else {
        copy.string_value = NULL;
      }
      break;
      
    case TOKEN_IDENTIFIER:
      if (token->identifier_value != NULL) {
        copy.identifier_value = safe_strdup(token->identifier_value);
        if (copy.identifier_value == NULL) {
          // Failed to allocate memory, return error token
          error_report(ERROR_MEMORY, "Failed to allocate memory for identifier token copy");
          copy.type = TOKEN_ERROR;
        }
      } else {
        copy.identifier_value = NULL;
      }
      break;
      
    case TOKEN_NUMBER:
      copy.number_value = token->number_value;
      break;
      
    default:
      // No type-specific data for other token types
      break;
  }
  
  return copy;
}

/**
 * @brief Create an invalid token with error message
 *
 * @param line Line number
 * @param column Column number
 * @param message Error message
 * @return Error token
 */
token_t token_create_error(size_t line, size_t column, const char* message) {
  token_t token;
  token.type = TOKEN_ERROR;
  token.start = NULL;
  token.length = 0;
  token.line = line;
  token.column = column;
  
  // Store error message in string_value
  if (message != NULL) {
    token.string_value = safe_strdup(message);
    if (token.string_value == NULL) {
      error_report(ERROR_MEMORY, "Failed to allocate memory for error token message");
    }
  } else {
    token.string_value = NULL;
  }
  
  return token;
}

/**
 * @brief Create a new lexer instance
 * 
 * @param source Source code to tokenize
 * @param length Length of source code in bytes
 * @param filename Source filename for error reporting
 * @return Pointer to created lexer or NULL on failure
 */
lexer_t* lexer_create(const char* source, size_t length, const char* filename) {
  if (source == NULL || length == 0) {
    error_report(ERROR_INVALID_ARGUMENT, "Invalid source code provided to lexer");
    return NULL;
  }

  // Check for excessive source code size
  if (length > LEXER_MAX_STRING_SIZE) {
    error_report(ERROR_INVALID_ARGUMENT, "Source code exceeds maximum size limit (%zu > %d bytes)",
                length, LEXER_MAX_STRING_SIZE);
    return NULL;
  }

  lexer_t* lexer = (lexer_t*)safe_malloc(sizeof(lexer_t));
  if (lexer == NULL) {
    error_report(ERROR_MEMORY, "Failed to allocate lexer");
    return NULL;
  }

  lexer->source = source;
  lexer->source_length = length;
  lexer->filename = filename ? filename : "<unknown>";
  lexer->current = source;
  lexer->line = 1;
  lexer->column = 1;
  lexer->has_peeked_token = false;
  lexer->has_error = false;

  // Register cleanup function
  error_register_cleanup((void (*)(void*))lexer_destroy, lexer);

  return lexer;
}

/**
 * @brief Destroy a lexer instance and free resources
 * 
 * @param lexer Lexer to destroy
 */
void lexer_destroy(lexer_t* lexer) {
  if (lexer == NULL) {
    return;
  }

  // Free the peeked token if present and it owns resources
  if (lexer->has_peeked_token) {
    token_destroy(&lexer->peeked_token);
    lexer->has_peeked_token = false;
  }

  free(lexer);
}

/**
 * @brief Check if current position is at end of source
 * 
 * @param lexer The lexer instance
 * @return true if at end, false otherwise
 */
static bool is_at_end(const lexer_t* lexer) {
  if (lexer == NULL || lexer->current == NULL || lexer->source == NULL) {
    return true;
  }
  
  ptrdiff_t diff = lexer->current - lexer->source;
  if (diff < 0 || (size_t)diff >= lexer->source_length) {
    return true;
  }
  
  return false;
}

/**
 * @brief Peek at the current character without advancing
 * 
 * @param lexer The lexer instance
 * @return The current character or '\0' if at end
 */
static char peek(const lexer_t* lexer) {
  if (is_at_end(lexer)) {
    return '\0';
  }
  return *lexer->current;
}

/**
 * @brief Peek at the next character without advancing
 * 
 * @param lexer The lexer instance
 * @return The next character or '\0' if at end
 */
static char peek_next(const lexer_t* lexer) {
  if (is_at_end(lexer)) {
    return '\0';
  }
  
  ptrdiff_t diff = lexer->current - lexer->source + 1;
  if (diff < 0 || (size_t)diff >= lexer->source_length) {
    return '\0';
  }
  
  return lexer->current[1];
}

/**
 * @brief Advance the current position by one character
 * 
 * @param lexer The lexer instance
 * @return The character that was consumed
 */
static char advance(lexer_t* lexer) {
  if (is_at_end(lexer)) {
    return '\0';
  }

  char c = *lexer->current++;
  
  if (c == '\n') {
    lexer->line++;
    lexer->column = 1;
  } else {
    lexer->column++;
  }
  
  return c;
}

/**
 * @brief Match the current character and advance if it matches
 * 
 * @param lexer The lexer instance
 * @param expected The character to match
 * @return true if the character matched and was consumed, false otherwise
 */
static bool match(lexer_t* lexer, char expected) {
  if (is_at_end(lexer) || *lexer->current != expected) {
    return false;
  }
  
  advance(lexer);
  return true;
}

/**
 * @brief Skip whitespace and comments
 * 
 * @param lexer The lexer instance
 */
static void skip_whitespace_and_comments(lexer_t* lexer) {
  while (true) {
    char c = peek(lexer);
    
    switch (c) {
      case ' ':
      case '\t':
      case '\r':
      case '\n':
        advance(lexer);
        break;
        
      case '/':
        if (peek_next(lexer) == '/') {
          // Line comment
          advance(lexer); // Skip first '/'
          advance(lexer); // Skip second '/'
          
          // Skip until end of line or end of file
          while (peek(lexer) != '\n' && !is_at_end(lexer)) {
            advance(lexer);
          }
        } else if (peek_next(lexer) == '*') {
          // Block comment
          advance(lexer); // Skip '/'
          advance(lexer); // Skip '*'
          
          // Skip until end of block comment or end of file
          while (!is_at_end(lexer)) {
            if (peek(lexer) == '*' && peek_next(lexer) == '/') {
              advance(lexer); // Skip '*'
              advance(lexer); // Skip '/'
              break;
            }
            
            // Handle nested comments (not supported, just track depth)
            if (peek(lexer) == '/' && peek_next(lexer) == '*') {
              error_report_at(ERROR_LEXICAL, lexer->filename, lexer->line, lexer->column,
                            "Nested block comments are not supported");
              lexer->has_error = true;
            }
            
            advance(lexer);
          }
          
          // Check if we reached the end without closing the comment
          if (is_at_end(lexer)) {
            error_report_at(ERROR_LEXICAL, lexer->filename, lexer->line, lexer->column,
                          "Unterminated block comment");
            lexer->has_error = true;
          }
        } else {
          // Not a comment
          return;
        }
        break;
        
      default:
        return;
    }
  }
}

/**
 * @brief Check if a character is valid in an identifier
 * 
 * @param c Character to check
 * @return true if valid, false otherwise
 */
static bool is_identifier_char(char c) {
  return isalnum(c) || c == '_';
}

/**
 * @brief Create a token of the specified type
 * 
 * @param lexer The lexer instance
 * @param type Token type
 * @param start Start of token in source
 * @param length Length of token
 * @return The created token
 */
static token_t make_token(lexer_t* lexer, token_type_t type, const char* start, size_t length) {
  token_t token;
  token.type = type;
  token.start = start;
  token.length = length;
  token.line = lexer->line;
  token.column = lexer->column - length;
  
  // Check for token length overflow
  if (length > MAX_TOKEN_LENGTH) {
    error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                  "Token length exceeds maximum allowed (%zu > %d)",
                  length, MAX_TOKEN_LENGTH);
    token.type = TOKEN_ERROR;
    token.string_value = NULL;
    lexer->has_error = true;
    return token;
  }

  // For tokens that need special handling
  switch (type) {
    case TOKEN_STRING:
      {
        // Check string length for overflow
        if (length > 2 && length - 2 > LEXER_MAX_STRING_SIZE) {
          error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                        "String is too long (%zu characters)",
                        length - 2);
          token.type = TOKEN_ERROR;
          token.string_value = NULL;
          lexer->has_error = true;
          return token;
        }
        
        // Allocate and copy string value without quotes and with escape sequences processed
        size_t max_len = length > 2 ? length - 2 : 0;  // -2 for quotes
        char* str = (char*)safe_malloc(max_len + 1);   // +1 for null terminator
        if (str == NULL) {
          error_report(ERROR_MEMORY, "Failed to allocate memory for string token");
          token.type = TOKEN_ERROR;
          token.string_value = NULL;
          lexer->has_error = true;
          return token;
        }
        
        size_t j = 0;
        // Start after the opening quote, end before the closing quote
        for (size_t i = 1; i < length - 1 && j < max_len; i++) {
          if (start[i] == '\\' && i + 1 < length - 1) {
            // Handle escape sequences
            i++;
            switch (start[i]) {
              case 'n': str[j++] = '\n'; break;
              case 't': str[j++] = '\t'; break;
              case 'r': str[j++] = '\r'; break;
              case '0': str[j++] = '\0'; break;
              case '"': str[j++] = '"'; break;
              case '\\': str[j++] = '\\'; break;
              default: 
                // Invalid escape sequence
                error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column + i,
                             "Invalid escape sequence '\\%c'", start[i]);
                str[j++] = start[i]; 
                break;
            }
          } else {
            str[j++] = start[i];
          }
          
          // Check for buffer overflow
          if (j >= max_len) {
            // This should never happen due to our previous check, but just in case
            error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                          "String buffer overflow");
            token.type = TOKEN_ERROR;
            free(str);
            token.string_value = NULL;
            lexer->has_error = true;
            return token;
          }
        }
        str[j] = '\0';
        token.string_value = str;
      }
      break;
      case TOKEN_IDENTIFIER:
      {
        // Allocate and copy identifier value
        char* id = (char*)safe_malloc(length + 1);
        if (id == NULL) {
          error_report(ERROR_MEMORY, "Failed to allocate memory for identifier token");
          token.type = TOKEN_ERROR;
          token.identifier_value = NULL;
          lexer->has_error = true;
          return token;
        }
        
        if (length > 0) {
          memcpy(id, start, length);
        }
        id[length] = '\0';
        token.identifier_value = id;
      }
      break;
      
    case TOKEN_NUMBER:
      {
        // Check for excessively long numbers
        if (length > 100) {
          error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                        "Number is too long (%zu digits)",
                        length);
          token.type = TOKEN_ERROR;
          token.number_value = 0;
          lexer->has_error = true;
          return token;
        }
        
        // Parse number value
        char* num_str = (char*)safe_malloc(length + 1);
        if (num_str == NULL) {
          error_report(ERROR_MEMORY, "Failed to allocate memory for number token");
          token.type = TOKEN_ERROR;
          token.number_value = 0;
          lexer->has_error = true;
          return token;
        }
        
        if (length > 0) {
          memcpy(num_str, start, length);
        }
        num_str[length] = '\0';
        
        // Determine base (decimal, hex, binary)
        int base = 10;
        char* parse_start = num_str;
        
        if (length > 2 && num_str[0] == '0' && (num_str[1] == 'x' || num_str[1] == 'X')) {
          base = 16;
          parse_start += 2;
        } else if (length > 2 && num_str[0] == '0' && (num_str[1] == 'b' || num_str[1] == 'B')) {
          base = 2;
          parse_start += 2;
        }
        
        // Handle floating point
        if (base == 10 && (strchr(num_str, '.') || strchr(num_str, 'e') || strchr(num_str, 'E'))) {
          // Reset errno before conversion
          errno = 0;
          
          // Convert to double
          char* endptr = NULL;
          double value = strtod(num_str, &endptr);
          
          // Check for conversion errors
          if (errno == ERANGE) {
            if (value == 0) {
              error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                           "Number underflow");
            } else {
              error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                           "Number overflow");
            }
            token.type = TOKEN_ERROR;
            free(num_str);
            token.number_value = 0;
            lexer->has_error = true;
            return token;
          }
          
          // Check if the entire string was converted
          if (endptr == num_str || *endptr != '\0') {
            error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                         "Invalid floating-point number");
            token.type = TOKEN_ERROR;
            free(num_str);
            token.number_value = 0;
            lexer->has_error = true;
            return token;
          }
          
          token.number_value = value;
        } else {
          // Integer value
          // Reset errno before conversion
          errno = 0;
          
          // Convert to integer
          char* endptr = NULL;
          long long value = strtoll(parse_start, &endptr, base);
          
          // Check for conversion errors
          if (errno == ERANGE) {
            error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                         "Integer overflow");
            token.type = TOKEN_ERROR;
            free(num_str);
            token.number_value = 0;
            lexer->has_error = true;
            return token;
          }
          
          // Check if the entire string was converted
          if (endptr == parse_start || *endptr != '\0') {
            error_report_at(ERROR_LEXICAL, lexer->filename, token.line, token.column,
                         "Invalid integer");
            token.type = TOKEN_ERROR;
            free(num_str);
            token.number_value = 0;
            lexer->has_error = true;
            return token;
          }
          
          token.number_value = (double)value;
        }
        
        free(num_str);
      }
      break;
      
    default:
      // Nothing special needed for other token types
      break;
  }

  return token;
}

/**
 * @brief Create an error token with a message
 * 
 * @param lexer The lexer instance
 * @param message Error message
 * @return The error token
 */
static token_t error_token(lexer_t* lexer, const char* message) {
  error_report_at(ERROR_LEXICAL, lexer->filename, lexer->line, lexer->column, "%s", message);
  lexer->has_error = true;
               
  token_t token;
  token.type = TOKEN_ERROR;
  token.start = lexer->current;
  token.length = 0;
  token.line = lexer->line;
  token.column = lexer->column;
  
  // Store error message
  if (message != NULL) {
    token.string_value = safe_strdup(message);
    if (token.string_value == NULL) {
      error_report(ERROR_MEMORY, "Failed to allocate memory for error message");
    }
  } else {
    token.string_value = NULL;
  }
  
  return token;
}

/**
 * @brief Scan an identifier token
 * 
 * @param lexer The lexer instance
 * @return The identifier token
 */
static token_t scan_identifier(lexer_t* lexer) {
  const char* start = lexer->current - 1; // -1 because we've already consumed the first character
  size_t length = 1;  // Start with 1 for the already consumed character
  
  // Count the length of the identifier
  while (is_identifier_char(peek(lexer))) {
    // Check for excessively long identifiers
    if (length >= MAX_TOKEN_LENGTH - 1) {
      return error_token(lexer, "Identifier too long");
    }
    
    advance(lexer);
    length++;
  }
  
  // Check if it's a keyword
  for (const keyword_t* k = keywords; k->keyword != NULL; k++) {
    size_t keyword_len = strlen(k->keyword);
    if (keyword_len == length && strncmp(start, k->keyword, length) == 0) {
      return make_token(lexer, k->type, start, length);
    }
  }
  
  // Not a keyword, so it's an identifier
  return make_token(lexer, TOKEN_IDENTIFIER, start, length);
}

/**
 * @brief Scan a number token
 * 
 * @param lexer The lexer instance
 * @return The number token
 */
static token_t scan_number(lexer_t* lexer) {
  const char* start = lexer->current - 1; // -1 because we've already consumed the first digit
  size_t length = 1;  // Start with 1 for the already consumed digit
  
  // Check for hex or binary prefix
  if (start[0] == '0' && !is_at_end(lexer)) {
    if (peek(lexer) == 'x' || peek(lexer) == 'X') {
      // Hexadecimal
      advance(lexer);
      length++;
      
      bool has_digits = false;
      while (isxdigit(peek(lexer))) {
        advance(lexer);
        length++;
        has_digits = true;
        
        // Check for excessive length
        if (length >= MAX_TOKEN_LENGTH - 1) {
          return error_token(lexer, "Number too long");
        }
      }
      
      if (!has_digits) {
        return error_token(lexer, "Expected hexadecimal digits after '0x'");
      }
    } else if (peek(lexer) == 'b' || peek(lexer) == 'B') {
      // Binary
      advance(lexer);
      length++;
      
      bool has_digits = false;
      while (peek(lexer) == '0' || peek(lexer) == '1' || peek(lexer) == '_') {
        if (peek(lexer) == '_') {
          // Skip underscore separators
          advance(lexer);
          continue;
        }
        
        advance(lexer);
        length++;
        has_digits = true;
        
        // Check for excessive length
        if (length >= MAX_TOKEN_LENGTH - 1) {
          return error_token(lexer, "Number too long");
        }
      }
      
      if (!has_digits) {
        return error_token(lexer, "Expected binary digits after '0b'");
      }
    }
  } else {
    // Decimal or floating point
    bool has_decimal = false;
    
    while (isdigit(peek(lexer)) || peek(lexer) == '_' || 
           (peek(lexer) == '.' && !has_decimal && isdigit(peek_next(lexer)))) {
      
      if (peek(lexer) == '_') {
        // Skip underscore separators
        advance(lexer);
        continue;
      }
      
      if (peek(lexer) == '.') {
        has_decimal = true;
      }
      
      advance(lexer);
      length++;
      
      // Check for excessive length
      if (length >= MAX_TOKEN_LENGTH - 1) {
        return error_token(lexer, "Number too long");
      }
    }
    
    // Check for scientific notation
    if (peek(lexer) == 'e' || peek(lexer) == 'E') {
      advance(lexer);
      length++;
      
      // Optional sign
      if (peek(lexer) == '+' || peek(lexer) == '-') {
        advance(lexer);
        length++;
      }
      
      if (!isdigit(peek(lexer))) {
        return error_token(lexer, "Expected exponent digits after 'e'");
      }
      
      while (isdigit(peek(lexer)) || peek(lexer) == '_') {
        if (peek(lexer) == '_') {
          // Skip underscore separators
          advance(lexer);
          continue;
        }
        
        advance(lexer);
        length++;
        
        // Check for excessive length
        if (length >= MAX_TOKEN_LENGTH - 1) {
          return error_token(lexer, "Number too long");
        }
      }
    }
  }
  
  // Check for invalid suffix
  if (is_identifier_char(peek(lexer))) {
    return error_token(lexer, "Invalid number suffix");
  }
  
  return make_token(lexer, TOKEN_NUMBER, start, length);
}

/**
 * @brief Scan a string token
 * 
 * @param lexer The lexer instance
 * @return The string token
 */
static token_t scan_string(lexer_t* lexer) {
  const char* start = lexer->current - 1; // -1 because we've already consumed the opening quote
  size_t length = 1;  // Start with 1 for the opening quote
  
  while (peek(lexer) != '"' && !is_at_end(lexer)) {
    if (peek(lexer) == '\n') {
      return error_token(lexer, "Unterminated string (newline in string)");
    }
    
    if (peek(lexer) == '\\') {
      advance(lexer); // Consume the backslash
      length++;
      
      // Check for valid escape sequence
      switch (peek(lexer)) {
        case '"':
        case '\\':
        case 'n':
        case 't':
        case 'r':
        case '0':
          break;
        default:
          // Invalid escape sequence, but continue parsing
          error_report_at(ERROR_LEXICAL, lexer->filename, lexer->line, lexer->column,
                       "Invalid escape sequence '\\%c'", peek(lexer));
          break;
      }
    }
    
    advance(lexer);
    length++;
    
    // Check for excessive string length
    if (length >= MAX_TOKEN_LENGTH - 1) {
      return error_token(lexer, "String too long");
    }
  }
  
  if (is_at_end(lexer)) {
    return error_token(lexer, "Unterminated string");
  }
  
  // Consume the closing quote
  advance(lexer);
  length++;
  
  return make_token(lexer, TOKEN_STRING, start, length);
}

/**
 * @brief Scan the next token from the source
 * 
 * @param lexer The lexer instance
 * @return The next token
 */
static token_t scan_token(lexer_t* lexer) {
  skip_whitespace_and_comments(lexer);
  
  if (is_at_end(lexer)) {
    return make_token(lexer, TOKEN_EOF, lexer->current, 0);
  }
  
  char c = advance(lexer);
  
  // Identifiers
  if (isalpha(c) || c == '_') {
    return scan_identifier(lexer);
  }
  
  // Numbers
  if (isdigit(c)) {
    return scan_number(lexer);
  }
  
  // Other tokens
  switch (c) {
    case '(': return make_token(lexer, TOKEN_LPAREN, lexer->current - 1, 1);
    case ')': return make_token(lexer, TOKEN_RPAREN, lexer->current - 1, 1);
    case '{': return make_token(lexer, TOKEN_LBRACE, lexer->current - 1, 1);
    case '}': return make_token(lexer, TOKEN_RBRACE, lexer->current - 1, 1);
    case '[': return make_token(lexer, TOKEN_LBRACKET, lexer->current - 1, 1);
    case ']': return make_token(lexer, TOKEN_RBRACKET, lexer->current - 1, 1);
    case ',': return make_token(lexer, TOKEN_COMMA, lexer->current - 1, 1);
    case ';': return make_token(lexer, TOKEN_SEMICOLON, lexer->current - 1, 1);
    case ':': return make_token(lexer, TOKEN_COLON, lexer->current - 1, 1);
    case '.': return make_token(lexer, TOKEN_DOT, lexer->current - 1, 1);
    case '=': return make_token(lexer, TOKEN_EQUALS, lexer->current - 1, 1);
    case '*': return make_token(lexer, TOKEN_ASTERISK, lexer->current - 1, 1);
    case '&': return make_token(lexer, TOKEN_AMPERSAND, lexer->current - 1, 1);
    case '<': return make_token(lexer, TOKEN_LESS, lexer->current - 1, 1);
    case '>': return make_token(lexer, TOKEN_GREATER, lexer->current - 1, 1);
    
    case '-':
      if (match(lexer, '>')) {
        return make_token(lexer, TOKEN_ARROW, lexer->current - 2, 2);
      }
      return error_token(lexer, "Unexpected character '-'");
    
    case '"': return scan_string(lexer);
    
    default: return error_token(lexer, "Unexpected character");
  }
}

/**
 * @brief Get the next token from the source
 * 
 * @param lexer The lexer instance
 * @return The next token
 */
token_t lexer_next_token(lexer_t* lexer) {
  if (lexer == NULL) {
    token_t error = token_create_error(0, 0, "Lexer is NULL");
    return error;
  }
  
  if (lexer->has_peeked_token) {
    lexer->has_peeked_token = false;
    token_t token = lexer->peeked_token;
    
    // Clear the peeked token without freeing its resources
    // since they've been moved to the returned token
    memset(&lexer->peeked_token, 0, sizeof(token_t));
    
    return token;
  }
  
  return scan_token(lexer);
}

/**
 * @brief Peek at the next token without consuming it
 * 
 * @param lexer The lexer instance
 * @return The next token
 */
token_t lexer_peek_token(lexer_t* lexer) {
  if (lexer == NULL) {
    token_t error = token_create_error(0, 0, "Lexer is NULL");
    return error;
  }
  
  if (lexer->has_peeked_token) {
    return lexer->peeked_token;
  }
  
  token_t token = scan_token(lexer);
  
  // Store a copy of the token
  lexer->peeked_token = token_copy(&token);
  lexer->has_peeked_token = true;
  
  return token;
}

/**
 * @brief Get the current line number in the source
 * 
 * @param lexer The lexer instance
 * @return Current line number
 */
size_t lexer_line(const lexer_t* lexer) {
  return lexer ? lexer->line : 0;
}

/**
 * @brief Get the current column number in the source
 * 
 * @param lexer The lexer instance
 * @return Current column number
 */
size_t lexer_column(const lexer_t* lexer) {
  return lexer ? lexer->column : 0;
}

/**
 * @brief Get the source filename
 * 
 * @param lexer The lexer instance
 * @return Source filename
 */
const char* lexer_filename(const lexer_t* lexer) {
  return lexer ? lexer->filename : "<unknown>";
}

/**
 * @brief Get if an error occurred during tokenization
 * 
 * @param lexer The lexer instance
 * @return true if error occurred, false otherwise
 */
bool lexer_has_error(const lexer_t* lexer) {
  return lexer ? lexer->has_error : true;
}