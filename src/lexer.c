/**
 * @file lexer.c
 * @brief Implementation of the lexical analyzer for HOIL
 * 
 * This file implements the lexer that breaks down HOIL source code
 * into tokens that can be processed by the parser.
 *
 * @author Generated by Claude
 * @date 2025-03-13
 */

#include "lexer.h"
#include "error_handling.h"
#include <string.h>
#include <ctype.h>
#include <assert.h>

/**
 * @brief Lexer structure definition
 */
struct lexer_t {
  const char* source;      /**< Source code being lexed */
  size_t source_length;    /**< Length of source code */
  const char* filename;    /**< Source filename for error reporting */
  
  const char* current;     /**< Current position in source */
  size_t line;             /**< Current line number */
  size_t column;           /**< Current column number */
  
  token_t peeked_token;    /**< Token buffer for peek operations */
  bool has_peeked_token;   /**< Whether there's a buffered token */
};

/**
 * @brief Keyword lookup entry
 */
typedef struct {
  const char* keyword;
  token_type_t type;
} keyword_t;

/**
 * @brief Table of HOIL keywords and their token types
 */
static const keyword_t keywords[] = {
  {"MODULE", TOKEN_MODULE},
  {"TYPE", TOKEN_TYPE},
  {"CONSTANT", TOKEN_CONSTANT},
  {"GLOBAL", TOKEN_GLOBAL},
  {"FUNCTION", TOKEN_FUNCTION},
  {"EXTERN", TOKEN_EXTERN},
  {"TARGET", TOKEN_TARGET},
  {"ENTRY", TOKEN_ENTRY},
  {"RET", TOKEN_RET},
  {"BR", TOKEN_BR},
  {"CALL", TOKEN_CALL},
  {"ADD", TOKEN_ADD},
  {"SUB", TOKEN_SUB},
  {"MUL", TOKEN_MUL},
  {"DIV", TOKEN_DIV},
  {"REM", TOKEN_REM},
  {"AND", TOKEN_AND},
  {"OR", TOKEN_OR},
  {"XOR", TOKEN_XOR},
  {"NOT", TOKEN_NOT},
  {"SHL", TOKEN_SHL},
  {"SHR", TOKEN_SHR},
  {"CMP_EQ", TOKEN_CMP_EQ},
  {"CMP_NE", TOKEN_CMP_NE},
  {"CMP_LT", TOKEN_CMP_LT},
  {"CMP_LE", TOKEN_CMP_LE},
  {"CMP_GT", TOKEN_CMP_GT},
  {"CMP_GE", TOKEN_CMP_GE},
  {"LOAD", TOKEN_LOAD},
  {"STORE", TOKEN_STORE},
  {"LEA", TOKEN_LEA},
  {"FENCE", TOKEN_FENCE},
  {"CONVERT", TOKEN_CONVERT},
  {"TRUNC", TOKEN_TRUNC},
  {"EXTEND", TOKEN_EXTEND},
  {"VADD", TOKEN_VADD},
  {"VDOT", TOKEN_VDOT},
  {"VSPLAT", TOKEN_VSPLAT},
  {"VLOAD", TOKEN_VLOAD},
  {"ATOMIC_ADD", TOKEN_ATOMIC_ADD},
  {"ATOMIC_CAS", TOKEN_ATOMIC_CAS},
  {"ALWAYS", TOKEN_ALWAYS},
  {NULL, TOKEN_EOF}
};

/**
 * @brief Convert token type to string for debugging
 * 
 * @param type Token type
 * @return String representation of token type
 */
const char* token_type_to_string(token_type_t type) {
  switch (type) {
    case TOKEN_EOF: return "EOF";
    case TOKEN_IDENTIFIER: return "IDENTIFIER";
    case TOKEN_NUMBER: return "NUMBER";
    case TOKEN_STRING: return "STRING";
    case TOKEN_MODULE: return "MODULE";
    case TOKEN_TYPE: return "TYPE";
    case TOKEN_CONSTANT: return "CONSTANT";
    case TOKEN_GLOBAL: return "GLOBAL";
    case TOKEN_FUNCTION: return "FUNCTION";
    case TOKEN_EXTERN: return "EXTERN";
    case TOKEN_TARGET: return "TARGET";
    case TOKEN_ENTRY: return "ENTRY";
    case TOKEN_RET: return "RET";
    case TOKEN_BR: return "BR";
    case TOKEN_CALL: return "CALL";
    case TOKEN_ADD: return "ADD";
    case TOKEN_SUB: return "SUB";
    case TOKEN_MUL: return "MUL";
    case TOKEN_DIV: return "DIV";
    case TOKEN_REM: return "REM";
    case TOKEN_AND: return "AND";
    case TOKEN_OR: return "OR";
    case TOKEN_XOR: return "XOR";
    case TOKEN_NOT: return "NOT";
    case TOKEN_SHL: return "SHL";
    case TOKEN_SHR: return "SHR";
    case TOKEN_CMP_EQ: return "CMP_EQ";
    case TOKEN_CMP_NE: return "CMP_NE";
    case TOKEN_CMP_LT: return "CMP_LT";
    case TOKEN_CMP_LE: return "CMP_LE";
    case TOKEN_CMP_GT: return "CMP_GT";
    case TOKEN_CMP_GE: return "CMP_GE";
    case TOKEN_LOAD: return "LOAD";
    case TOKEN_STORE: return "STORE";
    case TOKEN_LEA: return "LEA";
    case TOKEN_FENCE: return "FENCE";
    case TOKEN_CONVERT: return "CONVERT";
    case TOKEN_TRUNC: return "TRUNC";
    case TOKEN_EXTEND: return "EXTEND";
    case TOKEN_VADD: return "VADD";
    case TOKEN_VDOT: return "VDOT";
    case TOKEN_VSPLAT: return "VSPLAT";
    case TOKEN_VLOAD: return "VLOAD";
    case TOKEN_ATOMIC_ADD: return "ATOMIC_ADD";
    case TOKEN_ATOMIC_CAS: return "ATOMIC_CAS";
    case TOKEN_ALWAYS: return "ALWAYS";
    case TOKEN_LPAREN: return "(";
    case TOKEN_RPAREN: return ")";
    case TOKEN_LBRACE: return "{";
    case TOKEN_RBRACE: return "}";
    case TOKEN_LBRACKET: return "[";
    case TOKEN_RBRACKET: return "]";
    case TOKEN_COMMA: return ",";
    case TOKEN_SEMICOLON: return ";";
    case TOKEN_COLON: return ":";
    case TOKEN_DOT: return ".";
    case TOKEN_ARROW: return "->";
    case TOKEN_EQUALS: return "=";
    case TOKEN_ASTERISK: return "*";
    case TOKEN_AMPERSAND: return "&";
    case TOKEN_LESS: return "<";
    case TOKEN_GREATER: return ">";
    case TOKEN_ERROR: return "ERROR";
    default: return "UNKNOWN";
  }
}

/**
 * @brief Create a new lexer instance
 * 
 * @param source Source code to tokenize
 * @param length Length of source code in bytes
 * @param filename Source filename for error reporting
 * @return Pointer to created lexer or NULL on failure
 */
lexer_t* lexer_create(const char* source, size_t length, const char* filename) {
  if (source == NULL || length == 0) {
    error_report(ERROR_INVALID_ARGUMENT, "Invalid source code provided to lexer");
    return NULL;
  }

  lexer_t* lexer = (lexer_t*)malloc(sizeof(lexer_t));
  if (lexer == NULL) {
    error_report(ERROR_MEMORY, "Failed to allocate lexer");
    return NULL;
  }

  lexer->source = source;
  lexer->source_length = length;
  lexer->filename = filename ? filename : "<unknown>";
  lexer->current = source;
  lexer->line = 1;
  lexer->column = 1;
  lexer->has_peeked_token = false;

  return lexer;
}

/**
 * @brief Destroy a lexer instance and free resources
 * 
 * @param lexer Lexer to destroy
 */
void lexer_destroy(lexer_t* lexer) {
  if (lexer == NULL) {
    return;
  }

  // Free the peeked token if present and it owns resources
  if (lexer->has_peeked_token) {
    if (lexer->peeked_token.type == TOKEN_STRING && lexer->peeked_token.string_value) {
      free(lexer->peeked_token.string_value);
    } else if (lexer->peeked_token.type == TOKEN_IDENTIFIER && lexer->peeked_token.identifier_value) {
      free(lexer->peeked_token.identifier_value);
    }
  }

  free(lexer);
}

/**
 * @brief Check if current position is at end of source
 * 
 * @param lexer The lexer instance
 * @return true if at end, false otherwise
 */
static bool is_at_end(const lexer_t* lexer) {
  return (size_t)(lexer->current - lexer->source) >= lexer->source_length;
}

/**
 * @brief Peek at the current character without advancing
 * 
 * @param lexer The lexer instance
 * @return The current character or '\0' if at end
 */
static char peek(const lexer_t* lexer) {
  if (is_at_end(lexer)) {
    return '\0';
  }
  return *lexer->current;
}

/**
 * @brief Peek at the next character without advancing
 * 
 * @param lexer The lexer instance
 * @return The next character or '\0' if at end
 */
static char peek_next(const lexer_t* lexer) {
  if (is_at_end(lexer) || (size_t)(lexer->current - lexer->source + 1) >= lexer->source_length) {
    return '\0';
  }
  return lexer->current[1];
}

/**
 * @brief Advance the current position by one character
 * 
 * @param lexer The lexer instance
 * @return The character that was consumed
 */
static char advance(lexer_t* lexer) {
  if (is_at_end(lexer)) {
    return '\0';
  }

  char c = *lexer->current++;
  
  if (c == '\n') {
    lexer->line++;
    lexer->column = 1;
  } else {
    lexer->column++;
  }
  
  return c;
}

/**
 * @brief Match the current character and advance if it matches
 * 
 * @param lexer The lexer instance
 * @param expected The character to match
 * @return true if the character matched and was consumed, false otherwise
 */
static bool match(lexer_t* lexer, char expected) {
  if (is_at_end(lexer) || *lexer->current != expected) {
    return false;
  }
  
  advance(lexer);
  return true;
}

/**
 * @brief Skip whitespace and comments
 * 
 * @param lexer The lexer instance
 */
static void skip_whitespace_and_comments(lexer_t* lexer) {
  while (true) {
    char c = peek(lexer);
    
    switch (c) {
      case ' ':
      case '\t':
      case '\r':
      case '\n':
        advance(lexer);
        break;
        
      case '/':
        if (peek_next(lexer) == '/') {
          // Line comment
          advance(lexer); // Skip first '/'
          advance(lexer); // Skip second '/'
          
          // Skip until end of line or end of file
          while (peek(lexer) != '\n' && !is_at_end(lexer)) {
            advance(lexer);
          }
        } else if (peek_next(lexer) == '*') {
          // Block comment
          advance(lexer); // Skip '/'
          advance(lexer); // Skip '*'
          
          // Skip until end of block comment or end of file
          while (!is_at_end(lexer)) {
            if (peek(lexer) == '*' && peek_next(lexer) == '/') {
              advance(lexer); // Skip '*'
              advance(lexer); // Skip '/'
              break;
            }
            advance(lexer);
          }
        } else {
          // Not a comment
          return;
        }
        break;
        
      default:
        return;
    }
  }
}

/**
 * @brief Check if a character is valid in an identifier
 * 
 * @param c Character to check
 * @return true if valid, false otherwise
 */
static bool is_identifier_char(char c) {
  return isalnum(c) || c == '_';
}

/**
 * @brief Create a token of the specified type
 * 
 * @param lexer The lexer instance
 * @param type Token type
 * @param start Start of token in source
 * @param length Length of token
 * @return The created token
 */
static token_t make_token(lexer_t* lexer, token_type_t type, const char* start, size_t length) {
  token_t token;
  token.type = type;
  token.start = start;
  token.length = length;
  token.line = lexer->line;
  token.column = lexer->column - length;

  // For tokens that need special handling
  switch (type) {
    case TOKEN_STRING:
      {
        // Allocate and copy string value without quotes and with escape sequences processed
        char* str = (char*)malloc(length - 1); // -2 for quotes, +1 for null terminator
        if (str == NULL) {
          error_report(ERROR_MEMORY, "Failed to allocate memory for string token");
          token.type = TOKEN_ERROR;
          token.string_value = NULL;
          return token;
        }
        
        size_t j = 0;
        // Start after the opening quote, end before the closing quote
        for (size_t i = 1; i < length - 1; i++) {
          if (start[i] == '\\' && i + 1 < length - 1) {
            // Handle escape sequences
            i++;
            switch (start[i]) {
              case 'n': str[j++] = '\n'; break;
              case 't': str[j++] = '\t'; break;
              case 'r': str[j++] = '\r'; break;
              case '0': str[j++] = '\0'; break;
              case '"': str[j++] = '"'; break;
              case '\\': str[j++] = '\\'; break;
              default: str[j++] = start[i]; break;
            }
          } else {
            str[j++] = start[i];
          }
        }
        str[j] = '\0';
        token.string_value = str;
      }
      break;
      
    case TOKEN_IDENTIFIER:
      {
        // Allocate and copy identifier value
        char* id = (char*)malloc(length + 1);
        if (id == NULL) {
          error_report(ERROR_MEMORY, "Failed to allocate memory for identifier token");
          token.type = TOKEN_ERROR;
          token.identifier_value = NULL;
          return token;
        }
        
        memcpy(id, start, length);
        id[length] = '\0';
        token.identifier_value = id;
      }
      break;
      
    case TOKEN_NUMBER:
      {
        // Parse number value
        char* num_str = (char*)malloc(length + 1);
        if (num_str == NULL) {
          error_report(ERROR_MEMORY, "Failed to allocate memory for number token");
          token.type = TOKEN_ERROR;
          token.number_value = 0;
          return token;
        }
        
        memcpy(num_str, start, length);
        num_str[length] = '\0';
        
        // Determine base (decimal, hex, binary)
        int base = 10;
        char* parse_start = num_str;
        
        if (length > 2 && num_str[0] == '0' && (num_str[1] == 'x' || num_str[1] == 'X')) {
          base = 16;
          parse_start += 2;
        } else if (length > 2 && num_str[0] == '0' && (num_str[1] == 'b' || num_str[1] == 'B')) {
          base = 2;
          parse_start += 2;
        }
        
        // Handle floating point
        if (base == 10 && (strchr(num_str, '.') || strchr(num_str, 'e') || strchr(num_str, 'E'))) {
          token.number_value = strtod(num_str, NULL);
        } else {
          // Integer value
          token.number_value = (double)strtoll(parse_start, NULL, base);
        }
        
        free(num_str);
      }
      break;
      
    default:
      // Nothing special needed for other token types
      break;
  }

  return token;
}

/**
 * @brief Create an error token with a message
 * 
 * @param lexer The lexer instance
 * @param message Error message
 * @return The error token
 */
static token_t error_token(lexer_t* lexer, const char* message) {
  error_report(ERROR_LEXICAL, "%s:%zu:%zu: %s", 
               lexer->filename, lexer->line, lexer->column, message);
               
  token_t token;
  token.type = TOKEN_ERROR;
  token.start = lexer->current;
  token.length = 0;
  token.line = lexer->line;
  token.column = lexer->column;
  return token;
}

/**
 * @brief Scan an identifier token
 * 
 * @param lexer The lexer instance
 * @return The identifier token
 */
static token_t scan_identifier(lexer_t* lexer) {
  const char* start = lexer->current - 1; // -1 because we've already consumed the first character
  
  while (is_identifier_char(peek(lexer))) {
    advance(lexer);
  }
  
  size_t length = lexer->current - start;
  
  // Check if it's a keyword
  for (const keyword_t* k = keywords; k->keyword != NULL; k++) {
    if (strlen(k->keyword) == length && strncmp(start, k->keyword, length) == 0) {
      return make_token(lexer, k->type, start, length);
    }
  }
  
  // Not a keyword, so it's an identifier
  return make_token(lexer, TOKEN_IDENTIFIER, start, length);
}

/**
 * @brief Scan a number token
 * 
 * @param lexer The lexer instance
 * @return The number token
 */
static token_t scan_number(lexer_t* lexer) {
  const char* start = lexer->current - 1; // -1 because we've already consumed the first digit
  
  // Check for hex or binary prefix
  if (start[0] == '0' && !is_at_end(lexer)) {
    if (peek(lexer) == 'x' || peek(lexer) == 'X') {
      // Hexadecimal
      advance(lexer);
      while (isxdigit(peek(lexer))) {
        advance(lexer);
      }
    } else if (peek(lexer) == 'b' || peek(lexer) == 'B') {
      // Binary
      advance(lexer);
      while (peek(lexer) == '0' || peek(lexer) == '1' || peek(lexer) == '_') {
        advance(lexer);
      }
    }
  } else {
    // Decimal or floating point
    while (isdigit(peek(lexer))) {
      advance(lexer);
    }
    
    // Check for decimal point
    if (peek(lexer) == '.' && isdigit(peek_next(lexer))) {
      advance(lexer); // Consume the '.'
      
      while (isdigit(peek(lexer))) {
        advance(lexer);
      }
    }
    
    // Check for scientific notation
    if ((peek(lexer) == 'e' || peek(lexer) == 'E')) {
      advance(lexer);
      
      // Optional sign
      if (peek(lexer) == '+' || peek(lexer) == '-') {
        advance(lexer);
      }
      
      if (!isdigit(peek(lexer))) {
        return error_token(lexer, "Invalid scientific notation");
      }
      
      while (isdigit(peek(lexer))) {
        advance(lexer);
      }
    }
  }
  
  // Check for invalid suffix
  if (is_identifier_char(peek(lexer))) {
    return error_token(lexer, "Invalid number suffix");
  }
  
  return make_token(lexer, TOKEN_NUMBER, start, lexer->current - start);
}

/**
 * @brief Scan a string token
 * 
 * @param lexer The lexer instance
 * @return The string token
 */
static token_t scan_string(lexer_t* lexer) {
  const char* start = lexer->current - 1; // -1 because we've already consumed the opening quote
  
  while (peek(lexer) != '"' && !is_at_end(lexer)) {
    if (peek(lexer) == '\\' && peek_next(lexer) == '"') {
      advance(lexer); // Skip the backslash
    }
    advance(lexer);
  }
  
  if (is_at_end(lexer)) {
    return error_token(lexer, "Unterminated string");
  }
  
  // Consume the closing quote
  advance(lexer);
  
  return make_token(lexer, TOKEN_STRING, start, lexer->current - start);
}

/**
 * @brief Scan the next token from the source
 * 
 * @param lexer The lexer instance
 * @return The next token
 */
static token_t scan_token(lexer_t* lexer) {
  skip_whitespace_and_comments(lexer);
  
  if (is_at_end(lexer)) {
    return make_token(lexer, TOKEN_EOF, lexer->current, 0);
  }
  
  char c = advance(lexer);
  
  // Identifiers
  if (isalpha(c) || c == '_') {
    return scan_identifier(lexer);
  }
  
  // Numbers
  if (isdigit(c)) {
    return scan_number(lexer);
  }
  
  // Other tokens
  switch (c) {
    case '(': return make_token(lexer, TOKEN_LPAREN, lexer->current - 1, 1);
    case ')': return make_token(lexer, TOKEN_RPAREN, lexer->current - 1, 1);
    case '{': return make_token(lexer, TOKEN_LBRACE, lexer->current - 1, 1);
    case '}': return make_token(lexer, TOKEN_RBRACE, lexer->current - 1, 1);
    case '[': return make_token(lexer, TOKEN_LBRACKET, lexer->current - 1, 1);
    case ']': return make_token(lexer, TOKEN_RBRACKET, lexer->current - 1, 1);
    case ',': return make_token(lexer, TOKEN_COMMA, lexer->current - 1, 1);
    case ';': return make_token(lexer, TOKEN_SEMICOLON, lexer->current - 1, 1);
    case ':': return make_token(lexer, TOKEN_COLON, lexer->current - 1, 1);
    case '.': return make_token(lexer, TOKEN_DOT, lexer->current - 1, 1);
    case '=': return make_token(lexer, TOKEN_EQUALS, lexer->current - 1, 1);
    case '*': return make_token(lexer, TOKEN_ASTERISK, lexer->current - 1, 1);
    case '&': return make_token(lexer, TOKEN_AMPERSAND, lexer->current - 1, 1);
    case '<': return make_token(lexer, TOKEN_LESS, lexer->current - 1, 1);
    case '>': return make_token(lexer, TOKEN_GREATER, lexer->current - 1, 1);
    
    case '-':
      if (match(lexer, '>')) {
        return make_token(lexer, TOKEN_ARROW, lexer->current - 2, 2);
      }
      return error_token(lexer, "Unexpected character '-'");
    
    case '"': return scan_string(lexer);
    
    default: return error_token(lexer, "Unexpected character");
  }
}

/**
 * @brief Get the next token from the source
 * 
 * @param lexer The lexer instance
 * @return The next token
 */
token_t lexer_next_token(lexer_t* lexer) {
  if (lexer->has_peeked_token) {
    lexer->has_peeked_token = false;
    return lexer->peeked_token;
  }
  
  return scan_token(lexer);
}

/**
 * @brief Peek at the next token without consuming it
 * 
 * @param lexer The lexer instance
 * @return The next token
 */
token_t lexer_peek_token(lexer_t* lexer) {
  if (lexer->has_peeked_token) {
    return lexer->peeked_token;
  }
  
  lexer->peeked_token = scan_token(lexer);
  lexer->has_peeked_token = true;
  return lexer->peeked_token;
}

/**
 * @brief Get the current line number in the source
 * 
 * @param lexer The lexer instance
 * @return Current line number
 */
size_t lexer_line(const lexer_t* lexer) {
  return lexer->line;
}

/**
 * @brief Get the current column number in the source
 * 
 * @param lexer The lexer instance
 * @return Current column number
 */
size_t lexer_column(const lexer_t* lexer) {
  return lexer->column;
}

/**
 * @brief Get the source filename
 * 
 * @param lexer The lexer instance
 * @return Source filename
 */
const char* lexer_filename(const lexer_t* lexer) {
  return lexer->filename;
}